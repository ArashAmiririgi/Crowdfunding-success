{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 0. Import Libraries\n",
    "# -----------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, make_scorer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 1. Load & Merge Data\n",
    "# -----------------------\n",
    "novelty_df = pd.read_csv('novelty_scores.csv')\n",
    "coherence_df = pd.read_csv('coherence_scores.csv')\n",
    "distinctiveness_df = pd.read_csv('distinctiveness_scores.csv', index_col=0)\n",
    "\n",
    "# Prepare distinctiveness summary\n",
    "distinctiveness_summary = distinctiveness_df.reset_index()\n",
    "distinctiveness_summary.rename(columns={'index' : 'category'}, inplace = True)\n",
    "distinctiveness_summary = distinctiveness_summary[['category', 'final_distinctiveness_score']]\n",
    "\n",
    "\n",
    "# Merge features\n",
    "campaign_df = (\n",
    "    novelty_df\n",
    "    .merge(coherence_df[['category', 'coherence_score']], on='category', how='left')\n",
    "    .merge(distinctiveness_summary, on='category', how='left')\n",
    ")\n",
    "\n",
    "# Rename for clarity\n",
    "campaign_df.rename(columns={\n",
    "    'coherence_score': 'coherence',\n",
    "    'final_distinctiveness_score': 'distinctiveness',\n",
    "    'normalized_mean_top_10_tfidf': 'novelty'\n",
    "}, inplace=True)\n",
    "\n",
    "# -----------------------\n",
    "# 2. Date Processing & Campaign Duration\n",
    "# -----------------------\n",
    "for col in ['launch_date', 'success_date', 'failure_date']:\n",
    "    campaign_df[col] = pd.to_datetime(campaign_df[col])\n",
    "\n",
    "# Campaign duration in days\n",
    "campaign_df['campaign_duration'] = campaign_df.apply(\n",
    "    lambda row: (row['success_date'] - row['launch_date']).days\n",
    "    if pd.notna(row['success_date'])\n",
    "    else (row['failure_date'] - row['launch_date']).days,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Seasonal encoding\n",
    "campaign_df['day_number'] = (campaign_df['launch_date'] - campaign_df['launch_date'].min()).dt.days + 1\n",
    "campaign_df['annual_cycle_sin'] = np.sin(2 * np.pi * campaign_df['day_number'] / 365)\n",
    "campaign_df['annual_cycle_cos'] = np.cos(2 * np.pi * campaign_df['day_number'] / 365)\n",
    "\n",
    "# -----------------------\n",
    "# 3. Feature Engineering\n",
    "# -----------------------\n",
    "# Log-transform skewed numeric variables\n",
    "campaign_df['goal_log'] = np.log10(campaign_df['funding_goal'])\n",
    "campaign_df['description_length_log'] = np.log10(campaign_df['doc_length'])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "numeric_features = ['distinctiveness', 'coherence', 'novelty', 'goal_log', 'description_length_log', 'campaign_duration']\n",
    "campaign_df[numeric_features] = scaler.fit_transform(campaign_df[numeric_features])\n",
    "\n",
    "# Polynomial & interaction terms\n",
    "campaign_df['coherence_sq'] = campaign_df['coherence']**2\n",
    "campaign_df['distinctiveness_sq'] = campaign_df['distinctiveness']**2\n",
    "campaign_df['novelty_sq'] = campaign_df['novelty']**2\n",
    "\n",
    "campaign_df['distinctiveness_coherence'] = campaign_df['distinctiveness'] * campaign_df['coherence']\n",
    "campaign_df['distinctiveness_novelty'] = campaign_df['distinctiveness'] * campaign_df['novelty']\n",
    "campaign_df['novelty_coherence'] = campaign_df['novelty'] * campaign_df['coherence']\n",
    "\n",
    "# -----------------------\n",
    "# 4. Check Multicollinearity (VIF)\n",
    "# -----------------------\n",
    "X_vif = sm.add_constant(campaign_df[numeric_features + ['annual_cycle_sin', 'annual_cycle_cos']])\n",
    "vif_df = pd.DataFrame({\n",
    "    \"Variable\": X_vif.columns,\n",
    "    \"VIF\": [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "})\n",
    "print(\"VIF Scores:\\n\", vif_df)\n",
    "\n",
    "# -----------------------\n",
    "# 5. Prepare Modeling Data\n",
    "# -----------------------\n",
    "features = ['distinctiveness', 'coherence', 'novelty', 'annual_cycle_sin', 'annual_cycle_cos',\n",
    "            'goal_log', 'description_length_log', 'campaign_duration', 'parent_category',\n",
    "            'coherence_sq', 'distinctiveness_sq', 'novelty_sq',\n",
    "            'distinctiveness_coherence', 'distinctiveness_novelty', 'novelty_coherence']\n",
    "\n",
    "X = pd.get_dummies(campaign_df[features], columns=['parent_category'], drop_first=True, dtype=float)\n",
    "y = campaign_df['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# -----------------------\n",
    "# 6. Logistic Regression\n",
    "# -----------------------\n",
    "logreg = LogisticRegression(max_iter=500, class_weight='balanced')\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_lr = logreg.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr):.3f}\")\n",
    "print(f\"Logistic Regression F1: {f1_score(y_test, y_pred_lr):.3f}\")\n",
    "\n",
    "# Cross-validation\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_accuracy = cross_val_score(logreg, X, y, cv=cv, scoring='accuracy')\n",
    "cv_f1 = cross_val_score(logreg, X, y, cv=cv, scoring=f1_scorer)\n",
    "print(\"mean CV Accuracy:\", np.mean(cv_accuracy))\n",
    "print(\"mean CV F1:\", np.mean(cv_f1))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# 7. Random Forest\n",
    "# -----------------------\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=20, min_samples_leaf=10,\n",
    "                            max_features='sqrt', class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.3f}\")\n",
    "print(f\"Random Forest F1: {f1_score(y_test, y_pred_rf):.3f}\")\n",
    "\n",
    "# -----------------------\n",
    "# 8. XGBoost\n",
    "# -----------------------\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.01,\n",
    "                              max_depth=9, min_child_weight=3,\n",
    "                              colsample_bytree=0.5, subsample=0.5,\n",
    "                              objective='binary:logistic', random_state=42, use_label_encoder=False,\n",
    "                              eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(f\"XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb):.3f}\")\n",
    "print(f\"XGBoost F1: {f1_score(y_test, y_pred_xgb):.3f}\")\n",
    "\n",
    "# -----------------------\n",
    "# 9. Sample PDP (Partial Dependence) for Key Features\n",
    "# -----------------------\n",
    "def compute_pdp(var, model, X_train, num_points=50):\n",
    "    # Create a grid in standardized space with customized range for 'novelty'\n",
    "    min_scaled = X_train[var].min()\n",
    "    if var == 'novelty':\n",
    "        max_scaled = min(X_train[var].max(), 12)\n",
    "    else:\n",
    "        max_scaled = X_train[var].max()\n",
    "    grid_scaled = np.linspace(min_scaled, max_scaled, num_points)\n",
    "    \n",
    "    # Define the squared term and interaction terms for this variable\n",
    "    squared_term = var + '_sq'\n",
    "    interaction_terms = [\n",
    "        col for col in X_train.columns\n",
    "        if col != squared_term\n",
    "        and var in col.split('_')\n",
    "        and col != var\n",
    "    ]\n",
    "    \n",
    "    # List of features to update\n",
    "    features_to_set = [var, squared_term] + interaction_terms\n",
    "    \n",
    "    # Compute partial dependence\n",
    "    pd_values = []\n",
    "    for d_scaled in grid_scaled:\n",
    "        X_temp = X_train.copy()\n",
    "        X_temp[var] = d_scaled\n",
    "        X_temp[squared_term] = d_scaled ** 2\n",
    "        \n",
    "        for inter_term in interaction_terms:\n",
    "            other_var = [v for v in inter_term.split('_') if v != var][0]\n",
    "            X_temp[inter_term] = d_scaled * X_train[other_var]\n",
    "        \n",
    "        # Predict probabilities and average them\n",
    "        probs = model.predict_proba(X_temp)[:, 1]\n",
    "        pd_values.append(np.mean(probs))\n",
    "    \n",
    "    return grid_scaled, pd_values\n",
    "\n",
    "\n",
    "variables = ['distinctiveness', 'coherence', 'novelty']\n",
    "new_labels = ['Distinctiveness', 'Coherence', 'Novelty']\n",
    "\n",
    "# Set up the plot\n",
    "font = {'size': 12}\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    grid_scaled, pd_values = compute_pdp(var, logreg, X_train)\n",
    "    axes[i].plot(grid_scaled, pd_values, color='teal')\n",
    "    axes[i].set_xlabel(new_labels[i], fontdict=font)\n",
    "    axes[i].set_ylabel('Predicted Probability of Success', fontdict=font)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# 9. Sample PDP (Partial Dependence) for interaction of Features\n",
    "# -----------------------\n",
    "def compute_2d_pdp(interaction, var1, var2, model, X_train, num_points=5):\n",
    "    \"\"\"Compute 2D partial dependence values for an interaction term.\"\"\"\n",
    "    \n",
    "    # Create a meshgrid for the two variables with customized ranges\n",
    "    if var1 == 'novelty':\n",
    "        grid_var1 = np.linspace(X_train[var1].min(), min(X_train[var1].max(), 12), num_points)\n",
    "    else:\n",
    "        grid_var1 = np.linspace(X_train[var1].min(), X_train[var1].max(), num_points)\n",
    "    \n",
    "    if var2 == 'novelty':\n",
    "        grid_var2 = np.linspace(X_train[var2].min(), min(X_train[var2].max(), 12), num_points)\n",
    "    else:\n",
    "        grid_var2 = np.linspace(X_train[var2].min(), X_train[var2].max(), num_points)\n",
    "    \n",
    "    grid_var1, grid_var2 = np.meshgrid(grid_var1, grid_var2)\n",
    "    \n",
    "    # Array to store partial dependence values\n",
    "    pd_values = np.zeros_like(grid_var1)\n",
    "    \n",
    "    # Identify squared terms for var1 and var2\n",
    "    squared_terms = [var + '_sq' for var in [var1, var2] if var + '_sq' in X_train.columns]\n",
    "    \n",
    "    # Identify other interactions (exclude squared-term interactions)\n",
    "    other_interactions = [\n",
    "        col for col in X_train.columns\n",
    "        if '_' in col\n",
    "        and col != interaction\n",
    "        and (var1 in col.split('_') or var2 in col.split('_'))\n",
    "        and not col.endswith('_sq')\n",
    "    ]\n",
    "    \n",
    "    # Loop over the grid to compute partial dependence\n",
    "    for i in range(num_points):\n",
    "        for j in range(num_points):\n",
    "            val1 = grid_var1[i, j]\n",
    "            val2 = grid_var2[i, j]\n",
    "            X_temp = X_train.copy()\n",
    "            \n",
    "            # Update the two variables being varied\n",
    "            X_temp[var1] = val1\n",
    "            X_temp[var2] = val2\n",
    "            \n",
    "            # Update squared terms if they exist\n",
    "            if var1 + '_sq' in squared_terms:\n",
    "                X_temp[var1 + '_sq'] = val1 ** 2\n",
    "            if var2 + '_sq' in squared_terms:\n",
    "                X_temp[var2 + '_sq'] = val2 ** 2\n",
    "            \n",
    "            # Update the main interaction term\n",
    "            X_temp[interaction] = val1 * val2\n",
    "            \n",
    "            # Update other interactions involving var1 or var2\n",
    "            for other_inter in other_interactions:\n",
    "                vars_in_inter = [v for v in other_inter.split('_') if v != 'sq']\n",
    "                if var1 in vars_in_inter:\n",
    "                    other_var = [v for v in vars_in_inter if v != var1][0]\n",
    "                    X_temp[other_inter] = val1 * X_train[other_var]\n",
    "                elif var2 in vars_in_inter:\n",
    "                    other_var = [v for v in vars_in_inter if v != var2][0]\n",
    "                    X_temp[other_inter] = val2 * X_train[other_var]\n",
    "            \n",
    "            # Compute the average predicted probability\n",
    "            probs = model.predict_proba(X_temp)[:, 1]\n",
    "            pd_values[i, j] = np.mean(probs)\n",
    "    \n",
    "    return grid_var1, grid_var2, pd_values\n",
    "\n",
    "\n",
    "# Define the interactions to plot using your actual column names\n",
    "interactions = [\n",
    "    ('distinctiveness_coherence', 'distinctiveness', 'coherence'),\n",
    "    ('distinctiveness_novelty', 'distinctiveness', 'novelty'),\n",
    "    ('novelty_coherence', 'novelty', 'coherence')\n",
    "]\n",
    "\n",
    "# Create a figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "font = {'size': 12}\n",
    "\n",
    "# Generate and plot 2D PDPs for each interaction\n",
    "for i, (interaction, var1, var2) in enumerate(interactions):\n",
    "    grid_var1, grid_var2, pd_values = compute_2d_pdp(interaction, var1, var2, logreg, X_train)\n",
    "    ax = axes[i]\n",
    "    contour = ax.contourf(grid_var1, grid_var2, pd_values, levels=20, cmap='viridis')\n",
    "    ax.set_xlabel(var1.capitalize(), fontdict=font)\n",
    "    ax.set_ylabel(var2.capitalize(), fontdict=font)\n",
    "    fig.colorbar(contour, ax=ax, label='Predicted Probability of Success')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e8c839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
