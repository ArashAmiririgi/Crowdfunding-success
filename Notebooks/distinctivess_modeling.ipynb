{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec95cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Imports and NLTK setup\n",
    "# -----------------------\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from scipy.spatial import distance\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "NON_ALPHA_REGEX = re.compile(r'[^a-zA-Z\\s]')\n",
    "\n",
    "# -----------------------\n",
    "# 1. Load & Filter Data\n",
    "# -----------------------\n",
    "df = pd.read_csv(\n",
    "    'campaigns.csv',\n",
    "    usecols=['description', 'category', 'token_counts']\n",
    ")\n",
    "\n",
    "# Keep campaigns with enough tokens\n",
    "df = df[df['token_counts'] > 50]\n",
    "\n",
    "# Filter categories with enough samples\n",
    "MIN_SAMPLES_PER_CATEGORY = 500\n",
    "category_counts = df['category'].value_counts()\n",
    "valid_categories = category_counts[category_counts >= MIN_SAMPLES_PER_CATEGORY].index\n",
    "df = df[df['category'].isin(valid_categories)].reset_index(drop=True)\n",
    "\n",
    "# -----------------------\n",
    "# 2. Noun Phrase Extraction\n",
    "# -----------------------\n",
    "def extract_noun_phrases(text, grammar=None):\n",
    "    \"\"\"\n",
    "    Extract noun phrases from text using POS tagging and chunking.\n",
    "    \"\"\"\n",
    "    if grammar is None:\n",
    "        grammar = \"\"\"\n",
    "            NP: {<NN.?>+<NN.?>}\n",
    "            NP: {<JJ.?>+<NN.?>}\n",
    "        \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    chunk_parser = nltk.RegexpParser(grammar)\n",
    "    chunked = chunk_parser.parse(tagged)\n",
    "\n",
    "    return [\n",
    "        \" \".join(word for word, pos in subtree.leaves())\n",
    "        for subtree in chunked.subtrees(lambda t: t.label() == 'NP')\n",
    "    ]\n",
    "\n",
    "df['noun_phrases'] = df['description'].map(extract_noun_phrases)\n",
    "\n",
    "# -----------------------\n",
    "# 3. Phrase Cleaning\n",
    "# -----------------------\n",
    "def clean_phrases(phrases):\n",
    "    \"\"\"\n",
    "    Remove stopwords, punctuation, and non-alphabetic tokens from noun phrases.\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for phrase in phrases:\n",
    "        words = word_tokenize(phrase)\n",
    "        filtered = [\n",
    "            w for w in words\n",
    "            if w.lower() not in STOP_WORDS and not NON_ALPHA_REGEX.search(w)\n",
    "        ]\n",
    "        if filtered:\n",
    "            cleaned.append(\" \".join(filtered))\n",
    "    return cleaned\n",
    "\n",
    "df['noun_phrases'] = df['noun_phrases'].map(clean_phrases)\n",
    "\n",
    "# -----------------------\n",
    "# 4. Map phrases to campaigns\n",
    "# -----------------------\n",
    "phrase_to_campaigns = defaultdict(list)\n",
    "\n",
    "for idx, phrases in tqdm(df['noun_phrases'].items(), desc=\"Mapping phrases to campaigns\"):\n",
    "    for phrase in phrases:\n",
    "        if idx not in phrase_to_campaigns[phrase]:\n",
    "            phrase_to_campaigns[phrase].append(idx)\n",
    "\n",
    "phrase_df = pd.DataFrame(\n",
    "    list(phrase_to_campaigns.items()),\n",
    "    columns=['noun_phrase', 'campaign_indexes']\n",
    ")\n",
    "\n",
    "# Filter phrases that appear in at least 2 campaigns and at most 50% of all campaigns\n",
    "MAX_CAMPAIGNS_PER_PHRASE = len(df) // 2\n",
    "mask = phrase_df['campaign_indexes'].apply(lambda idxs: 2 <= len(idxs) <= MAX_CAMPAIGNS_PER_PHRASE)\n",
    "phrase_df = phrase_df[mask].reset_index(drop=True)\n",
    "\n",
    "# -----------------------\n",
    "# 5. Map phrases to categories\n",
    "# -----------------------\n",
    "campaign_categories = df['category']\n",
    "\n",
    "def get_categories_for_phrase(indexes):\n",
    "    return [campaign_categories.iloc[i] for i in indexes]\n",
    "\n",
    "phrase_df['categories'] = phrase_df['campaign_indexes'].map(get_categories_for_phrase)\n",
    "\n",
    "# -----------------------\n",
    "# 6. Map categories to phrases\n",
    "# -----------------------\n",
    "category_to_phrases = defaultdict(list)\n",
    "\n",
    "for idx, categories in phrase_df['categories'].items():\n",
    "    for category in categories:\n",
    "        category_to_phrases[category].append(idx)\n",
    "\n",
    "category_df = pd.DataFrame(\n",
    "    list(category_to_phrases.items()),\n",
    "    columns=['category', 'phrase_indexes']\n",
    ")\n",
    "\n",
    "def get_phrases_for_category(indexes):\n",
    "    return [phrase_df['noun_phrase'].iloc[i] for i in indexes]\n",
    "\n",
    "category_df['noun_phrases'] = category_df['phrase_indexes'].map(get_phrases_for_category)\n",
    "\n",
    "# -----------------------\n",
    "# 7. Compute distribution vectors for each category\n",
    "# -----------------------\n",
    "num_phrases = len(phrase_df)\n",
    "\n",
    "def category_distribution(indexes):\n",
    "    vec = [0] * num_phrases\n",
    "    for idx in indexes:\n",
    "        vec[idx] += 1\n",
    "    return vec\n",
    "\n",
    "category_df['distribution'] = category_df['phrase_indexes'].map(category_distribution)\n",
    "\n",
    "# -----------------------\n",
    "# 8. Compute Jensenâ€“Shannon distances between categories\n",
    "# -----------------------\n",
    "categories = category_df['category'].tolist()\n",
    "distributions = category_df['distribution'].tolist()\n",
    "\n",
    "category_distinctiveness_df = pd.DataFrame(0.0, index=categories, columns=categories)\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(categories)):\n",
    "        dist = distance.jensenshannon(distributions[i], distributions[j]) ** 2\n",
    "        category_distinctiveness_df.iloc[i, j] = dist\n",
    "\n",
    "# -----------------------\n",
    "# 9. Calculate final distinctiveness score per category\n",
    "# -----------------------\n",
    "TOTAL_CATEGORIES = len(categories)  # number of categories\n",
    "final_distinctiveness_scores = category_distinctiveness_df.sum(axis=1) / TOTAL_CATEGORIES\n",
    "\n",
    "# Add the final score as a new column\n",
    "category_distinctiveness_df['final_distinctiveness_score'] = final_distinctiveness_scores\n",
    "\n",
    "# -----------------------\n",
    "# 10. Output Result\n",
    "# -----------------------\n",
    "category_distinctiveness_df.to_csv('distinctiveness_scores.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
