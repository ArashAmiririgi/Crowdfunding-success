{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Imports and NLTK setup\n",
    "# -----------------------\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "NON_ALPHA_REGEX = re.compile(r'[^a-zA-Z\\s]')\n",
    "\n",
    "# -----------------------\n",
    "# 1. Load & Filter Data\n",
    "# -----------------------\n",
    "df = pd.read_csv('campaigns.csv', usecols=['description', 'category', 'token_counts', 'launch_date', 'success', 'funding_goal', 'parent_category', 'failure_date', 'success_date'])\n",
    "\n",
    "# Filter campaigns with sufficient token counts for reliable analysis\n",
    "MIN_TOKEN_COUNT = 50\n",
    "df = df[df['token_counts'] > MIN_TOKEN_COUNT]\n",
    "\n",
    "# Keep only categories with enough campaigns for statistical power\n",
    "MIN_CAMPAIGNS_PER_CATEGORY = 500\n",
    "valid_categories = df['category'].value_counts()\n",
    "valid_categories = valid_categories[valid_categories >= MIN_CAMPAIGNS_PER_CATEGORY].index\n",
    "df = df[df['category'].isin(valid_categories)].reset_index(drop=True)\n",
    "\n",
    "# -----------------------\n",
    "# 2. Extract Noun Phrases from Descriptions\n",
    "# -----------------------\n",
    "def extract_noun_phrases(text, grammar=None):\n",
    "    \"\"\"\n",
    "    Extract noun phrases (NP) from text using POS tagging and chunking.\n",
    "    Grammar:\n",
    "        NP = consecutive nouns or adjective + noun sequences\n",
    "    \"\"\"\n",
    "    if grammar is None:\n",
    "        grammar = r\"\"\"\n",
    "            NP: {<NN.*>+}\n",
    "            NP: {<JJ.*>+<NN.*>+}\n",
    "        \"\"\"\n",
    "    tokens = word_tokenize(text.lower())  # lowercase for uniformity\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    chunk_parser = nltk.RegexpParser(grammar)\n",
    "    tree = chunk_parser.parse(tagged_tokens)\n",
    "\n",
    "    # Extract NP as joined strings\n",
    "    noun_phrases = [\n",
    "        \" \".join(word for word, pos in subtree.leaves())\n",
    "        for subtree in tree.subtrees()\n",
    "        if subtree.label() == 'NP'\n",
    "    ]\n",
    "    return noun_phrases\n",
    "\n",
    "df['noun_phrases'] = df['description'].map(extract_noun_phrases)\n",
    "\n",
    "# -----------------------\n",
    "# 3. Clean Noun Phrases\n",
    "# -----------------------\n",
    "def clean_phrases(phrases):\n",
    "    \"\"\"\n",
    "    Remove stopwords, punctuation, and non-alphabetic tokens from noun phrases.\n",
    "    \"\"\"\n",
    "    cleaned_phrases = []\n",
    "    for phrase in phrases:\n",
    "        words = word_tokenize(phrase)\n",
    "        filtered_words = [\n",
    "            w for w in words\n",
    "            if w not in STOP_WORDS and not NON_ALPHA_REGEX.search(w)\n",
    "        ]\n",
    "        if filtered_words:\n",
    "            cleaned_phrases.append(\" \".join(filtered_words))\n",
    "    return cleaned_phrases\n",
    "\n",
    "df['cleaned_noun_phrases'] = df['noun_phrases'].map(clean_phrases)\n",
    "\n",
    "# -----------------------\n",
    "# 4. Merge Cleaned Noun Phrases for Vectorization\n",
    "# -----------------------\n",
    "def merge_phrases_for_vectorization(phrases):\n",
    "    return \" \".join([phrase.replace(\" \", \"_\") for phrase in phrases])\n",
    "\n",
    "df['merged_noun_phrases'] = df['cleaned_noun_phrases'].map(merge_phrases_for_vectorization)\n",
    "\n",
    "# Filter out empty merged phrases\n",
    "df = df[df['merged_noun_phrases'].str.strip() != ''].reset_index(drop=True)\n",
    "\n",
    "# -----------------------\n",
    "# 5. Compute TF-IDF Matrix for Noun Phrases\n",
    "# -----------------------\n",
    "tfidf_vectorizer = TfidfVectorizer(norm=None, binary=False, smooth_idf=False)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['merged_noun_phrases'])\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# -----------------------\n",
    "# 6. Extract Non-zero TF-IDF Values per Document\n",
    "# -----------------------\n",
    "rows, cols = tfidf_matrix.nonzero()\n",
    "values = tfidf_matrix.data\n",
    "\n",
    "# Map row index to list of non-zero tfidf values\n",
    "row_to_tfidf_values = defaultdict(list)\n",
    "for row_idx, val in zip(rows, values):\n",
    "    row_to_tfidf_values[row_idx].append(val)\n",
    "\n",
    "# Prepare DataFrame column with non-zero tfidf values\n",
    "df['non_zero_tfidf_values'] = [row_to_tfidf_values.get(i, []) for i in range(tfidf_matrix.shape[0])]\n",
    "\n",
    "# -----------------------\n",
    "# 7. Calculate Novelty Features Based on TF-IDF Scores\n",
    "# -----------------------\n",
    "# Compute description length in words\n",
    "df['doc_length'] = df['description'].map(lambda x: len(x.split()))\n",
    "\n",
    "# Get top 10 TF-IDF values per document\n",
    "def get_top_n_tfidf_scores(tfidf_list, top_n=10):\n",
    "    if not tfidf_list:\n",
    "        return []\n",
    "    return sorted(tfidf_list, reverse=True)[:top_n]\n",
    "\n",
    "# Compute top N TF-IDF scores per document\n",
    "df[\"top_10_tfidf_scores\"] = df[\"non_zero_tfidf_values\"].map(get_top_n_tfidf_scores)\n",
    "\n",
    "# Compute mean of top N TF-IDF scores\n",
    "df[\"mean_top_10_tfidf\"] = df[\"top_10_tfidf_scores\"].apply(\n",
    "    lambda scores: np.mean(scores) if scores else 0.0\n",
    ")\n",
    "\n",
    "# Normalize mean TF-IDF by log10 of description length to adjust for length effects\n",
    "def normalize_mean_tfidf(mean_tfidf, doc_length):\n",
    "    if doc_length > 1:\n",
    "        return mean_tfidf / np.log10(doc_length)\n",
    "    return 0.0\n",
    "\n",
    "df[\"normalized_mean_top_10_tfidf\"] = df.apply(\n",
    "    lambda row: normalize_mean_tfidf(row[\"mean_top_10_tfidf\"], row[\"doc_length\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df.to_csv('novelty_scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
